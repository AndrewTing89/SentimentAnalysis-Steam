{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd68761-0706-450c-bae5-2bb7514c2630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: sentiment-analysis-steam\n",
      "Region: us-west1\n",
      "GCS Bucket: steam-reviews-bucket-0\n",
      "Training Data URI: gs://steam-reviews-bucket-0/steam_reviews_cleaned.csv\n",
      "Training Docker Image Name: steam-lr-review-trainer\n",
      "Full Training Docker Image URI: gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest\n",
      "Updated property [core/project].\n",
      "\n",
      "✅ gcloud project set to sentiment-analysis-steam\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Configuration Variables for LR+TF-IDF Training\n",
    "\n",
    "PROJECT_ID = \"sentiment-analysis-steam\" # Replace with your actual GCP Project ID\n",
    "REGION = \"us-west1\"                    # Must match your GCS bucket region and Vertex AI job region\n",
    "BUCKET_NAME = \"steam-reviews-bucket-0\" # Your GCS bucket name\n",
    "\n",
    "# Path to your cleaned data for training\n",
    "DATA_URI = f\"gs://{BUCKET_NAME}/steam_reviews_cleaned.csv\"\n",
    "\n",
    "# --- Specifics for your LR+TF-IDF Training Container ---\n",
    "TRAINING_IMAGE_NAME = \"steam-lr-review-trainer\" # The name you've given to your training image\n",
    "FULL_TRAINING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{TRAINING_IMAGE_NAME}:latest\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"GCS Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Training Data URI: {DATA_URI}\")\n",
    "print(f\"Training Docker Image Name: {TRAINING_IMAGE_NAME}\")\n",
    "print(f\"Full Training Docker Image URI: {FULL_TRAINING_IMAGE_URI}\")\n",
    "\n",
    "# Set the gcloud project config (important for subsequent gcloud commands)\n",
    "# This command needs to be run in a cell where you can execute shell commands.\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "print(f\"\\n✅ gcloud project set to {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "525b5eaf-ea76-47af-afa2-2483a070bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and pushing Docker image: gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest...\n",
      "Creating temporary archive of 8 file(s) totalling 45.0 KiB before compression.\n",
      "Uploading tarball of [./lr_tfidf_trainer] to [gs://sentiment-analysis-steam_cloudbuild/source/1751949673.286384-d80f7de4488f4c5db563c6bf2fd50467.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/sentiment-analysis-steam/locations/global/builds/91be929d-4b7e-4b8c-b9d7-c0f16b281fad].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/91be929d-4b7e-4b8c-b9d7-c0f16b281fad?project=1063155306158 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"91be929d-4b7e-4b8c-b9d7-c0f16b281fad\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://sentiment-analysis-steam_cloudbuild/source/1751949673.286384-d80f7de4488f4c5db563c6bf2fd50467.tgz#1751949673490901\n",
      "Copying gs://sentiment-analysis-steam_cloudbuild/source/1751949673.286384-d80f7de4488f4c5db563c6bf2fd50467.tgz#1751949673490901...\n",
      "/ [1 files][  9.3 KiB/  9.3 KiB]                                                \n",
      "Operation completed over 1 objects/9.3 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  54.27kB\n",
      "Step 1/6 : FROM python:3.9-slim\n",
      "3.9-slim: Pulling from library/python\n",
      "3da95a905ed5: Pulling fs layer\n",
      "ae17c88c7d53: Pulling fs layer\n",
      "08ebcf91c620: Pulling fs layer\n",
      "9f1673b82500: Pulling fs layer\n",
      "9f1673b82500: Waiting\n",
      "ae17c88c7d53: Verifying Checksum\n",
      "ae17c88c7d53: Download complete\n",
      "08ebcf91c620: Verifying Checksum\n",
      "08ebcf91c620: Download complete\n",
      "3da95a905ed5: Verifying Checksum\n",
      "3da95a905ed5: Download complete\n",
      "9f1673b82500: Verifying Checksum\n",
      "9f1673b82500: Download complete\n",
      "3da95a905ed5: Pull complete\n",
      "ae17c88c7d53: Pull complete\n",
      "08ebcf91c620: Pull complete\n",
      "9f1673b82500: Pull complete\n",
      "Digest: sha256:c2a0feb07dedbf91498883c2f8e1e5201e95c91d413e21c3bea780c8aad8e6a7\n",
      "Status: Downloaded newer image for python:3.9-slim\n",
      " ---> a1ec6a2ef164\n",
      "Step 2/6 : WORKDIR /app\n",
      " ---> Running in 92b1bcc719b6\n",
      "Removing intermediate container 92b1bcc719b6\n",
      " ---> d4a51d534d40\n",
      "Step 3/6 : COPY requirements.txt .\n",
      " ---> 40d63b1c2ebe\n",
      "Step 4/6 : RUN pip install --no-cache-dir -r requirements.txt\n",
      " ---> Running in 2e09fb3e6a5a\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 47.6 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 129.4 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 206.7 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2025.5.1-py2.py3-none-any.whl (36 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-3.2.0-py3-none-any.whl (176 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.1/176.1 kB 213.8 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/19.5 MB 178.1 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 347.8/347.8 kB 222.2 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 509.2/509.2 kB 214.6 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 220.6 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 177.7 MB/s eta 0:00:00\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 179.2 MB/s eta 0:00:00\n",
      "Collecting fsspec==2025.5.1\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 169.8 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.8/64.8 kB 171.6 MB/s eta 0:00:00\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 216.1/216.1 kB 211.5 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3\n",
      "  Downloading google_crc32c-1.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.2\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-api-core<3.0.0,>=2.15.0\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.8/160.8 kB 193.8 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media<3.0.0,>=2.7.2\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 156.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (239 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.2/239.2 kB 196.6 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 179.6 MB/s eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.1/225.1 kB 222.3 MB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (200 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.2/200.2 kB 195.1 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 327.3/327.3 kB 194.8 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.5/294.5 kB 214.8 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0,>=1.22.3\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 161.2 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.1/321.1 kB 203.2 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 205.9 MB/s eta 0:00:00\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.8/129.8 kB 195.6 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.7/157.7 kB 207.8 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 179.5 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.5/149.5 kB 187.1 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting typing-extensions>=4.2\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.9/43.9 kB 107.6 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 125.8 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.1/160.1 kB 169.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, threadpoolctl, six, pyasn1, protobuf, propcache, oauthlib, numpy, joblib, idna, google-crc32c, fsspec, frozenlist, decorator, charset_normalizer, certifi, cachetools, attrs, async-timeout, aiohappyeyeballs, scipy, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, multidict, googleapis-common-protos, google-resumable-media, aiosignal, yarl, scikit-learn, requests-oauthlib, pandas, google-auth, google-auth-oauthlib, google-api-core, aiohttp, google-cloud-core, google-cloud-storage, gcsfs\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 cachetools-5.5.2 certifi-2025.6.15 charset_normalizer-3.4.2 decorator-5.2.1 frozenlist-1.7.0 fsspec-2025.5.1 gcsfs-2025.5.1 google-api-core-2.25.1 google-auth-2.40.3 google-auth-oauthlib-1.2.2 google-cloud-core-2.4.3 google-cloud-storage-3.2.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 idna-3.10 joblib-1.5.1 multidict-6.6.3 numpy-2.0.2 oauthlib-3.3.1 pandas-2.3.1 propcache-0.3.2 proto-plus-1.26.1 protobuf-6.31.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.4 requests-oauthlib-2.0.0 rsa-4.9.1 scikit-learn-1.6.1 scipy-1.13.1 six-1.17.0 threadpoolctl-3.6.0 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0 yarl-1.20.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 2e09fb3e6a5a\n",
      " ---> b2c6d0b1c6e5\n",
      "Step 5/6 : COPY task.py .\n",
      " ---> c61bd7d253c3\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Running in f92b57587529\n",
      "Removing intermediate container f92b57587529\n",
      " ---> 2afdedd6a2b5\n",
      "Successfully built 2afdedd6a2b5\n",
      "Successfully tagged gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest\n",
      "The push refers to repository [gcr.io/sentiment-analysis-steam/steam-lr-review-trainer]\n",
      "1ca3146355ea: Preparing\n",
      "12bb0e0f1058: Preparing\n",
      "9576179694d9: Preparing\n",
      "09c39be64975: Preparing\n",
      "13242f53ac09: Preparing\n",
      "3d8533b4dc1f: Preparing\n",
      "7ff2f4499221: Preparing\n",
      "1bb35e8b4de1: Preparing\n",
      "3d8533b4dc1f: Waiting\n",
      "7ff2f4499221: Waiting\n",
      "1bb35e8b4de1: Waiting\n",
      "13242f53ac09: Layer already exists\n",
      "3d8533b4dc1f: Layer already exists\n",
      "09c39be64975: Pushed\n",
      "7ff2f4499221: Layer already exists\n",
      "1ca3146355ea: Pushed\n",
      "9576179694d9: Pushed\n",
      "1bb35e8b4de1: Layer already exists\n",
      "12bb0e0f1058: Pushed\n",
      "latest: digest: sha256:a6ceb882df70176e1d4167c542b87e0d39c6d1b30f2db4e88f9b188df5bb6682 size: 1994\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                  IMAGES                                                             STATUS\n",
      "91be929d-4b7e-4b8c-b9d7-c0f16b281fad  2025-07-08T04:41:13+00:00  1M23S     gs://sentiment-analysis-steam_cloudbuild/source/1751949673.286384-d80f7de4488f4c5db563c6bf2fd50467.tgz  gcr.io/sentiment-analysis-steam/steam-lr-review-trainer (+1 more)  SUCCESS\n",
      "\n",
      "✅ Docker image built and pushed: gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Build and Push LR+TF-IDF Training Docker Image\n",
    "\n",
    "# IMPORTANT: Ensure your current working directory in the notebook terminal\n",
    "# is the parent directory of your 'lr_tfidf_trainer' folder.\n",
    "# For example, if your 'lr_tfidf_trainer' folder is directly in your\n",
    "# JupyterLab root (e.g., /home/jupyter/SentimentAnalysis-Steam/),\n",
    "# then your notebook's CWD should be that root.\n",
    "# You can use !pwd and !ls -F to check. Use %cd to navigate if needed.\n",
    "\n",
    "print(f\"Building and pushing Docker image: {FULL_TRAINING_IMAGE_URI}...\")\n",
    "\n",
    "# This command executes Cloud Build to build your image from the Dockerfile\n",
    "# located in the './lr_tfidf_trainer' directory and push it to GCR.\n",
    "!gcloud builds submit --tag {FULL_TRAINING_IMAGE_URI} ./lr_tfidf_trainer\n",
    "\n",
    "print(f\"\\n✅ Docker image built and pushed: {FULL_TRAINING_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be14744-f61a-41b0-8b0b-85b7f7c87632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting training job using image: gcr.io/sentiment-analysis-steam/steam-lr-review-trainer:latest...\n",
      "Training Output directory:\n",
      "gs://steam-reviews-bucket-0/vertex_ai_staging/aiplatform-custom-training-2025-07-08-04:02:30.354 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/5881656308247035904?project=1063155306158\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/7351940849095147520?project=1063155306158\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1063155306158/locations/us-west1/trainingPipelines/5881656308247035904 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed with:\ncode: 5\nmessage: \"There are no files under \\\"gs://steam-reviews-bucket-0/vertex_ai_staging/aiplatform-custom-training-2025-07-08-04:02:30.354/model\\\" to copy.\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mCustomContainerTrainingJob(\n\u001b[1;32m     16\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr-tfidf-steam-sentiment-training\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     container_uri\u001b[38;5;241m=\u001b[39mTRAINING_IMAGE_URI,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     model_serving_container_image_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Use this for scikit-learn\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmitting training job using image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAINING_IMAGE_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of worker replicas (1 for single instance)\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn1-standard-4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# A general purpose machine type. Adjust if you need more CPU/RAM.\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# For TF-IDF, ample RAM is often more important than raw CPU count.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# Consider 'n1-standard-8' or 'n1-standard-16' if your dataset is very large.\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No accelerator_type/count needed here as LR+TF-IDF runs on CPU\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- Pass Arguments to your task.py script ---\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--project-id=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_ID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--bucket-name=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--data-uri=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_URI\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Training job submitted! Job ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39mresource_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can monitor its progress in the Google Cloud Console: Vertex AI > Training > Custom jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:4979\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, tpu_topology, scheduling_strategy, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, max_wait_duration, psc_interface_config)\u001b[0m\n\u001b[1;32m   4960\u001b[0m service_account \u001b[38;5;241m=\u001b[39m service_account \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mservice_account\n\u001b[1;32m   4962\u001b[0m worker_pool_specs, managed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_and_validate_run(\n\u001b[1;32m   4963\u001b[0m     model_display_name\u001b[38;5;241m=\u001b[39mmodel_display_name,\n\u001b[1;32m   4964\u001b[0m     model_labels\u001b[38;5;241m=\u001b[39mmodel_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4976\u001b[0m     reservation_affinity_values\u001b[38;5;241m=\u001b[39mreservation_affinity_values,\n\u001b[1;32m   4977\u001b[0m )\n\u001b[0;32m-> 4979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_pool_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_pool_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanaged_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4989\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4990\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5005\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_web_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_web_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5006\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_dashboard_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_dashboard_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_container_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction_server_container_uri\u001b[49m\n\u001b[1;32m   5010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m   5011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   5012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5013\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpsc_interface_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpsc_interface_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:5778\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob._run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries, persistent_resource_id, scheduling_strategy, max_wait_duration, psc_interface_config)\u001b[0m\n\u001b[1;32m   5753\u001b[0m             spec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainerSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   5754\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value}\n\u001b[1;32m   5755\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m environment_variables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5756\u001b[0m             ]\n\u001b[1;32m   5758\u001b[0m (\n\u001b[1;32m   5759\u001b[0m     training_task_inputs,\n\u001b[1;32m   5760\u001b[0m     base_output_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5775\u001b[0m     psc_interface_config\u001b[38;5;241m=\u001b[39mpsc_interface_config,\n\u001b[1;32m   5776\u001b[0m )\n\u001b[0;32m-> 5778\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_definition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_destination_uri_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:855\u001b[0m, in \u001b[0;36m_TrainingJob._run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m training_pipeline\n\u001b[1;32m    853\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Training:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dashboard_uri())\n\u001b[0;32m--> 855\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining did not produce a Managed Model returning None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_upload_fail_string\n\u001b[1;32m    861\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:942\u001b[0m, in \u001b[0;36m_TrainingJob._get_model\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to get and instantiate the Model to Upload.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    RuntimeError: If Training failed.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Pipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed. No model available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    947\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:985\u001b[0m, in \u001b[0;36m_TrainingJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_callback()\n\u001b[1;32m    983\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(_JOB_WAIT_TIME)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mmodel_to_upload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:1002\u001b[0m, in \u001b[0;36m_TrainingJob._raise_failure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to raise failure if TrainingPipeline fails.\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m    RuntimeError: If training failed.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m code_pb2\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training failed with:\ncode: 5\nmessage: \"There are no files under \\\"gs://steam-reviews-bucket-0/vertex_ai_staging/aiplatform-custom-training-2025-07-08-04:02:30.354/model\\\" to copy.\"\n"
     ]
    }
   ],
   "source": [
    "# executes (runs) that container image as a training job on Vertex AI.\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# --- Configuration for your Training Job ---\n",
    "PROJECT_ID = \"sentiment-analysis-steam\" # Replace with your actual project ID\n",
    "REGION = \"us-west1\"            # Choose your preferred GCP region\n",
    "BUCKET_NAME = \"steam-reviews-bucket-0\" # Your GCS bucket name\n",
    "DATA_URI = f\"gs://{BUCKET_NAME}/steam_reviews_cleaned.csv\" # Path to your cleaned data\n",
    "TRAINING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/steam-lr-review-trainer:latest\" # Image from Step 5\n",
    "\n",
    "# --- Initialize Vertex AI SDK ---\n",
    "# 'staging_bucket' is used by Vertex AI for temporary files during job execution.\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}/vertex_ai_staging\")\n",
    "\n",
    "# --- Define the Custom Training Job ---\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name='lr-tfidf-steam-sentiment-training',\n",
    "    container_uri=TRAINING_IMAGE_URI,\n",
    "    # This is the pre-built serving container for scikit-learn models.\n",
    "    # It will be used if you deploy the model directly via this 'model' object later.\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\" # Use this for scikit-learn\n",
    ")\n",
    "\n",
    "print(f\"Submitting training job using image: {TRAINING_IMAGE_URI}...\")\n",
    "model = job.run(\n",
    "    replica_count=1, # Number of worker replicas (1 for single instance)\n",
    "    machine_type='n1-standard-4', # A general purpose machine type. Adjust if you need more CPU/RAM.\n",
    "                                   # For TF-IDF, ample RAM is often more important than raw CPU count.\n",
    "                                   # Consider 'n1-standard-8' or 'n1-standard-16' if your dataset is very large.\n",
    "    # No accelerator_type/count needed here as LR+TF-IDF runs on CPU\n",
    "    \n",
    "    # --- Pass Arguments to your task.py script ---\n",
    "    args=[\n",
    "        f\"--project-id={PROJECT_ID}\",\n",
    "        f\"--bucket-name={BUCKET_NAME}\",\n",
    "        f\"--data-uri={DATA_URI}\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Training job submitted! Job ID: {job.resource_name.split('/')[-1]}\")\n",
    "print(f\"You can monitor its progress in the Google Cloud Console: Vertex AI > Training > Custom jobs\")\n",
    "\n",
    "# Once the job completes, 'model' object will contain information about the saved model artifact.\n",
    "# You can inspect model.uri to see where the model was saved.\n",
    "print(f\"\\nModel artifacts will be saved to: {model.uri} (and sub-timestamped folders)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bb1c0-52e7-41a4-b4ea-cb0e8dec8c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
