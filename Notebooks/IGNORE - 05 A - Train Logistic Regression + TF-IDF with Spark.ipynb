{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1f8a8-7700-4ea2-9ba1-8d3d9e438326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SteamReviewsTuning\").getOrCreate()\n",
    "\n",
    "# -- Configuration: Using your specified column names --\n",
    "TEXT_COLUMN = \"review_text\"\n",
    "LABEL_COLUMN = \"review_score\"\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 2. Load Data and add label/weight columns\n",
    "print(\"üìñ Loading data and preparing for training...\")\n",
    "file_path = \"gs://steam-reviews-bucket-0/steam_reviews_cleaned.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"label\", col(LABEL_COLUMN).cast(\"double\")).na.drop()\n",
    "\n",
    "balance_ratio = df.groupBy(\"label\").count()\n",
    "count_total = df.count()\n",
    "count_class_0 = balance_ratio.filter(col(\"label\") == 0).select(\"count\").collect()[0][0]\n",
    "count_class_1 = balance_ratio.filter(col(\"label\") == 1).select(\"count\").collect()[0][0]\n",
    "weight_class_0 = count_total / (2.0 * count_class_0)\n",
    "weight_class_1 = count_total / (2.0 * count_class_1)\n",
    "df = df.withColumn(\"classWeight\", when(col(\"label\") == 1, weight_class_1).otherwise(weight_class_0))\n",
    "\n",
    "\n",
    "# 3. Create the base pipeline (before tuning)\n",
    "tokenizer = Tokenizer(inputCol=TEXT_COLUMN, outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"classWeight\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "\n",
    "# 4. Set up the Hyperparameter Grid\n",
    "print(\"üõ†Ô∏è Setting up the hyperparameter grid for tuning...\")\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(hashingTF.numFeatures, [10000, 50000])\n",
    "             .addGrid(lr.regParam, [0.1, 0.01])\n",
    "             .build())\n",
    "\n",
    "\n",
    "# 5. Set up the Cross-Validator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "\n",
    "# 6. Split data and run the Cross-Validation to find the best model\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"\\nüß† Starting cross-validation... (This will take a long time)\")\n",
    "cvModel = crossval.fit(training_data)\n",
    "print(\"‚úÖ Cross-validation complete. Best model found.\")\n",
    "\n",
    "\n",
    "# 7. Use the BEST model found by the CrossValidator for evaluation\n",
    "print(\"\\nüìä Evaluating the BEST model on the test set...\")\n",
    "bestModel = cvModel.bestModel\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# --- General Evaluation ---\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nüèÖ ROC AUC of Best Model: {roc_auc:.4f}\")\n",
    "\n",
    "# --- Detailed Per-Class Evaluation ---\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Metrics for Negative Reviews (Class 0)\n",
    "precision_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "recall_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "f1_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\", multi_evaluator.metricLabel: 0.0})\n",
    "\n",
    "# Metrics for Positive Reviews (Class 1)\n",
    "precision_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 1.0})\n",
    "recall_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 1.0})\n",
    "f1_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\", multi_evaluator.metricLabel: 1.0})\n",
    "\n",
    "print(\"\\n--- Metrics for Negative Reviews (Class 0) ---\")\n",
    "print(f\"üéØ Precision: {precision_0:.4f}\")\n",
    "print(f\"üîÅ Recall:    {recall_0:.4f}\")\n",
    "print(f\"üéØ F1 Score:  {f1_0:.4f}\")\n",
    "\n",
    "print(\"\\n--- Metrics for Positive Reviews (Class 1) ---\")\n",
    "print(f\"üéØ Precision: {precision_1:.4f}\")\n",
    "print(f\"üîÅ Recall:    {recall_1:.4f}\")\n",
    "print(f\"üéØ F1 Score:  {f1_1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\\nüìã Confusion Matrix\")\n",
    "print(\"Rows: True Label, Columns: Predicted Label\")\n",
    "predictions.groupBy('label').pivot('prediction', [0.0, 1.0]).count().na.fill(0).show()\n",
    "\n",
    "\n",
    "# 8. Save the best model in both Spark and MLeap formats\n",
    "print(\"\\nüíæ Saving the best model to GCS...\")\n",
    "\n",
    "# -- Original Spark format (good for batch jobs) --\n",
    "spark_model_path = \"gs://steam-reviews-bucket-0/models/spark_lr_model_tuned\"\n",
    "bestModel.write().overwrite().save(spark_model_path)\n",
    "print(f\"‚úÖ Best Spark model saved to: {spark_model_path}\")\n",
    "\n",
    "# -- MLeap format (for serving) --\n",
    "# Note: MLeap needs a schema definition from a sample DataFrame\n",
    "df_schema = df.select(TEXT_COLUMN) # Your model only needs the text column as input\n",
    "\n",
    "mleap_model_path = \"gs://steam-reviews-bucket-0/models/mleap_lr_model_tuned.zip\"\n",
    "bestModel.serializeToBundle(f\"jar:file:{mleap_model_path}\", df_schema)\n",
    "print(f\"‚úÖ Best MLeap model successfully saved to: {mleap_model_path}\")\n",
    "\n",
    "# 9. Stop the SparkSession\n",
    "print(\"\\nüõë Stopping the Spark session.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae3d26-8cec-4a49-b0a7-a04e10bdda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (after your training and evaluation is complete) ...\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "# 8. Save the best model in both Spark and MLeap formats\n",
    "print(\"\\nüíæ Saving the best model to GCS...\")\n",
    "\n",
    "# -- Original Spark format (good for batch jobs) --\n",
    "spark_model_path = \"gs://steam-reviews-bucket-0/models/spark_lr_model_tuned\"\n",
    "bestModel.write().overwrite().save(spark_model_path)\n",
    "print(f\"‚úÖ Best Spark model saved to: {spark_model_path}\")\n",
    "\n",
    "# -- MLeap format (for serving) --\n",
    "# Note: MLeap needs a schema definition from a sample DataFrame\n",
    "df_schema = df.select(TEXT_COLUMN) # Your model only needs the text column as input\n",
    "\n",
    "mleap_model_path = \"gs://steam-reviews-bucket-0/models/mleap_lr_model_tuned.zip\"\n",
    "bestModel.serializeToBundle(f\"jar:file:{mleap_model_path}\", df_schema)\n",
    "print(f\"‚úÖ Best MLeap model successfully saved to: {mleap_model_path}\")\n",
    "\n",
    "# 9. Stop the SparkSession\n",
    "print(\"\\nüõë Stopping the Spark session.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38694ffe-5022-4eea-9e4a-72d7435ffad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-runtime-0000898c75af",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "steam‚Äêspark‚Äêtemplate on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000898c75af"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
