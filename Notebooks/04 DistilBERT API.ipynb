{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc1c8e8-b254-418c-a1ef-8ac8dc323257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install SDKs if the environment is fresh\n",
    "!pip install -q --upgrade google-cloud-aiplatform huggingface_hub\n",
    "!pip install -q --upgrade google-cloud-secret-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab398f7-a38d-466b-a3ed-b3d3df6c4516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 Pull the token from Secret Manager (first cell in the CPU notebook)\n",
    "\n",
    "from google.cloud import secretmanager, aiplatform\n",
    "import datetime, os\n",
    "\n",
    "PROJECT_ID = \"sentiment-analysis-steam\"          # ‚Üê your GCP project\n",
    "REGION     = \"us-central1\"\n",
    "\n",
    "def get_secret(secret_id: str, project_id: str) -> str:\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    name   = f\"projects/{project_id}/secrets/{secret_id}/versions/latest\"\n",
    "    return client.access_secret_version(name=name).payload.data.decode(\"utf-8\")\n",
    "\n",
    "HF_TOKEN = get_secret(\"HF_TOKEN\", PROJECT_ID)     # üîí fetched securely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "295b9f6e-fb0d-44e2-b60a-414daa30db2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/1063155306158/locations/us-central1/models/3829769423846113280/operations/7084012508473720832\n",
      "Model created. Resource name: projects/1063155306158/locations/us-central1/models/3829769423846113280@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1063155306158/locations/us-central1/models/3829769423846113280@1')\n",
      "üÜî Vertex Model: projects/1063155306158/locations/us-central1/models/3829769423846113280\n"
     ]
    }
   ],
   "source": [
    "# 2 Upload the CPU model artifact\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "CONTAINER = (\n",
    "    \"us-docker.pkg.dev/deeplearning-platform-release/\"\n",
    "    \"gcr.io/huggingface-pytorch-inference-\"\n",
    "    \"cpu.2-3.transformers.4-46.ubuntu2204.py311\"    # CPU image (no ‚Äúcu121‚Äù)\n",
    ")\n",
    "\n",
    "TIMESTAMP   = datetime.datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME  = f\"steam-distilbert-cpu-{TIMESTAMP}\"\n",
    "REPO_ID     = \"andrewting89/steam-distilbert\"       # HF repo you pushed earlier\n",
    "\n",
    "cpu_model = aiplatform.Model.upload(\n",
    "    display_name  = MODEL_NAME,\n",
    "    serving_container_image_uri = CONTAINER,\n",
    "    serving_container_environment_variables = {\n",
    "        \"HF_MODEL_ID\": REPO_ID,\n",
    "        \"HF_TASK\":     \"text-classification\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN,\n",
    "    },\n",
    "    sync=True,\n",
    ")\n",
    "print(\"üÜî Vertex Model:\", cpu_model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe6a610-579b-435d-8a9b-994ceb244849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n",
      "Deploy Endpoint model backing LRO: projects/1063155306158/locations/us-central1/endpoints/265181313898643456/operations/1733736151157571584\n",
      "Endpoint model deployed. Resource name: projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n",
      "‚úÖ Endpoint live on CPU: projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n"
     ]
    }
   ],
   "source": [
    "# 3 Deploy on a CPU endpoint\n",
    "ENDPOINT_NAME = \"steam-sentiment-endpoint\"\n",
    "\n",
    "# Re-use the endpoint if it exists, otherwise create a new one\n",
    "eps = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_NAME}\"', location=REGION)\n",
    "endpoint = eps[0] if eps else aiplatform.Endpoint.create(\n",
    "    display_name=ENDPOINT_NAME, sync=True)\n",
    "\n",
    "# (optional) wipe any old, failed revisions\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "cpu_model.deploy(\n",
    "    endpoint            = endpoint,\n",
    "    machine_type        = \"n1-standard-4\",   # 4-vCPU CPU VM\n",
    "    min_replica_count   = 1,\n",
    "    max_replica_count   = 1,\n",
    "    traffic_percentage  = 100,\n",
    "    sync=True,                               # wait until READY\n",
    ")\n",
    "print(\"‚úÖ Endpoint live on CPU:\", endpoint.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44dc358e-a111-4c81-8ba9-78bd1ccde72b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n",
      "265181313898643456\n"
     ]
    }
   ],
   "source": [
    "# Confirm correct endpoint_id\n",
    "\n",
    "print(endpoint.resource_name)\n",
    "# ‚Üí projects/123456789012/locations/us-central1/endpoints/9876543210987654321\n",
    "\n",
    "ENDPOINT_ID = endpoint.resource_name.split(\"/\")[-1]\n",
    "print(ENDPOINT_ID)        # ‚Üí 9876543210987654321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66420638-7f9e-41c8-ba3f-5b7a7ddac09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ü≥ raw response: Prediction(predictions=[{'score': 0.9148386120796204, 'label': 'POSITIVE'}], deployed_model_id='6228560748025479168', metadata=None, model_version_id='1', model_resource_name='projects/1063155306158/locations/us-central1/models/3829769423846113280', explanations=None)\n",
      "\n",
      "‚úÖ Model says: POSITIVE  (confidence ‚âà 91.48%)\n"
     ]
    }
   ],
   "source": [
    "# 4 Smoke-test\n",
    "TEST_TEXT = \"This game is so bug-free and the story is amazing!\"\n",
    "\n",
    "prediction = endpoint.predict(\n",
    "    instances=[{\"text\": TEST_TEXT}]\n",
    ")\n",
    "\n",
    "print(\"‚Ü≥ raw response:\", prediction)\n",
    "\n",
    "# nice-format\n",
    "label = prediction.predictions[0][\"label\"]\n",
    "score = prediction.predictions[0][\"score\"]\n",
    "print(f\"\\n‚úÖ Model says: {label}  (confidence ‚âà {score:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f87c43-aeae-44c6-b388-7cb945123c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying all replicas ‚Ä¶\n",
      "Undeploying Endpoint model: projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n",
      "Undeploy Endpoint model backing LRO: projects/1063155306158/locations/us-central1/endpoints/265181313898643456/operations/7102449119448268800\n",
      "Endpoint model undeployed. Resource name: projects/1063155306158/locations/us-central1/endpoints/265181313898643456\n",
      "‚úÖ Endpoint replicas = 0; node-hour billing stopped.\n",
      "No running VMs matched the prefix; nothing to stop.\n"
     ]
    }
   ],
   "source": [
    "# Tear down API Endpoint and Cluster/VMs\n",
    "\n",
    "# ‚îÄ‚îÄ FULL TEAR-DOWN CELL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from google.cloud import aiplatform, compute_v1\n",
    "from google.api_core.exceptions import NotFound\n",
    "import re\n",
    "\n",
    "PROJECT   = \"sentiment-analysis-steam\"\n",
    "REGION    = \"us-central1\"\n",
    "ENDPOINT  = \"projects/1063155306158/locations/us-central1/endpoints/265181313898643456\"\n",
    "CLUSTER_PREFIX = r\"steam-sentiment-cluster\"     # regex for VM names to stop\n",
    "\n",
    "# 1) Scale Vertex endpoint to zero -------------------------------------\n",
    "try:\n",
    "    endpoint = aiplatform.Endpoint(ENDPOINT)\n",
    "    if endpoint.gca_resource.deployed_models:\n",
    "        print(\"Undeploying all replicas ‚Ä¶\")\n",
    "        endpoint.undeploy_all(sync=True)\n",
    "        print(\"‚úÖ Endpoint replicas = 0; node-hour billing stopped.\")\n",
    "    else:\n",
    "        print(\"Endpoint already at 0 replicas.\")\n",
    "except NotFound:\n",
    "    print(\"Endpoint not found (already deleted).\")\n",
    "\n",
    "# 2) Stop any matching Compute-Engine VMs ------------------------------\n",
    "compute = compute_v1.InstancesClient()\n",
    "request = compute_v1.AggregatedListInstancesRequest(\n",
    "    project=PROJECT,\n",
    "    filter=\"status = RUNNING\"\n",
    ")\n",
    "\n",
    "stopped = []\n",
    "pattern = re.compile(CLUSTER_PREFIX)\n",
    "for zone_path, resp in compute.aggregated_list(request=request):\n",
    "    zone = zone_path.split(\"/\")[-1]\n",
    "    for inst in getattr(resp, \"instances\", []):\n",
    "        if pattern.match(inst.name):\n",
    "            print(f\"Stopping VM {inst.name} in {zone} ‚Ä¶\")\n",
    "            compute.stop(project=PROJECT, zone=zone, instance=inst.name)\n",
    "            stopped.append(inst.name)\n",
    "\n",
    "if stopped:\n",
    "    print(\"‚úÖ All cluster VMs are stopping; billing ends when they reach TERMINATED.\")\n",
    "else:\n",
    "    print(\"No running VMs matched the prefix; nothing to stop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681c38d-91dd-4981-805a-db92db8e4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Smoke-test\n",
    "TEST_TEXT = \"This game is so bug-free and the story is amazing!\"\n",
    "\n",
    "prediction = endpoint.predict(\n",
    "    instances=[{\"text\": TEST_TEXT}]\n",
    ")\n",
    "\n",
    "print(\"‚Ü≥ raw response:\", prediction)\n",
    "\n",
    "# nice-format\n",
    "label = prediction.predictions[0][\"label\"]\n",
    "score = prediction.predictions[0][\"score\"]\n",
    "print(f\"\\n‚úÖ Model says: {label}  (confidence ‚âà {score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf58877-ed4a-4f35-81cb-4bcc54e6d47d",
   "metadata": {},
   "source": [
    "## Make sure all clusters/API endpoints are shut down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e2fed5-74a7-4ed3-b27c-aa76fc8ea2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade google-cloud-compute tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7faf4a9c-5317-4a22-a402-78a8cef17e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vertex endpoint has **0** deployed replicas ‚Üí $0 node-hour right now.\n",
      "\n",
      "‚ö†Ô∏è  These VMs are still RUNNING (incurring charges):\n",
      "instance                  zone           machine_type\n",
      "------------------------  -------------  --------------\n",
      "instance-20250629-185113  us-central1-a  n2-highmem-8\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Billing-sanity cell ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from google.cloud import aiplatform, compute_v1\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "PROJECT  = \"sentiment-analysis-steam\"      # adjust if needed\n",
    "REGION   = \"us-central1\"\n",
    "# Numeric endpoint ID or full resource name:\n",
    "ENDPOINT_ID = \"265181313898643456\"\n",
    "\n",
    "# 1) Endpoint status -------------------------------------------------\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint = aiplatform.Endpoint(f\"projects/{PROJECT}/locations/{REGION}/endpoints/{ENDPOINT_ID}\")\n",
    "\n",
    "deployed = endpoint.gca_resource.deployed_models\n",
    "if not deployed:\n",
    "    print(\"‚úÖ Vertex endpoint has **0** deployed replicas ‚Üí $0 node-hour right now.\\n\")\n",
    "else:\n",
    "    rows = []\n",
    "    for dm in deployed:\n",
    "        res = dm.dedicated_resources\n",
    "        machine = res.machine_spec.machine_type.split(\"/\")[-1]\n",
    "        rows.append([dm.id, machine,\n",
    "                     res.min_replica_count, res.max_replica_count])\n",
    "    print(\"‚ö†Ô∏è  Endpoint is billing for the following replicas:\")\n",
    "    print(tabulate(rows, headers=[\"deployed_model_id\", \"machine_type\", \"min_repl\", \"max_repl\"]))\n",
    "    print()\n",
    "\n",
    "# 2) Running Compute-Engine VMs -------------------------------------\n",
    "compute = compute_v1.InstancesClient()\n",
    "request = compute_v1.AggregatedListInstancesRequest(\n",
    "    project=PROJECT,\n",
    "    filter=\"status = RUNNING\"\n",
    ")\n",
    "\n",
    "running = []\n",
    "for zone, resp in compute.aggregated_list(request=request):\n",
    "    for inst in getattr(resp, \"instances\", []):\n",
    "        z = zone.split(\"/\")[-1]\n",
    "        running.append([inst.name, z, inst.machine_type.split('/')[-1]])\n",
    "\n",
    "if not running:\n",
    "    print(\"‚úÖ No Compute-Engine instances are RUNNING ‚Üí no VM charges.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  These VMs are still RUNNING (incurring charges):\")\n",
    "    print(tabulate(running, headers=[\"instance\", \"zone\", \"machine_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90634660-6601-4834-b527-c955b6c0db70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
