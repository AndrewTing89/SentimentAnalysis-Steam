{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1f8a8-7700-4ea2-9ba1-8d3d9e438326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 06:29:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading data and preparing for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Setting up the hyperparameter grid for tuning...\n",
      "\n",
      "üß† Starting cross-validation... (This will take a long time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================>                        (9 + 7) / 16]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SteamReviewsTuning\").getOrCreate()\n",
    "\n",
    "# -- Configuration: Using your specified column names --\n",
    "TEXT_COLUMN = \"review_text\"\n",
    "LABEL_COLUMN = \"review_score\"\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 2. Load Data and add label/weight columns\n",
    "print(\"üìñ Loading data and preparing for training...\")\n",
    "file_path = \"gs://steam-reviews-bucket-0/steam_reviews_cleaned.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"label\", col(LABEL_COLUMN).cast(\"double\")).na.drop()\n",
    "\n",
    "balance_ratio = df.groupBy(\"label\").count()\n",
    "count_total = df.count()\n",
    "count_class_0 = balance_ratio.filter(col(\"label\") == 0).select(\"count\").collect()[0][0]\n",
    "count_class_1 = balance_ratio.filter(col(\"label\") == 1).select(\"count\").collect()[0][0]\n",
    "weight_class_0 = count_total / (2.0 * count_class_0)\n",
    "weight_class_1 = count_total / (2.0 * count_class_1)\n",
    "df = df.withColumn(\"classWeight\", when(col(\"label\") == 1, weight_class_1).otherwise(weight_class_0))\n",
    "\n",
    "\n",
    "# 3. Create the base pipeline (before tuning)\n",
    "tokenizer = Tokenizer(inputCol=TEXT_COLUMN, outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"classWeight\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "\n",
    "# 4. Set up the Hyperparameter Grid\n",
    "print(\"üõ†Ô∏è Setting up the hyperparameter grid for tuning...\")\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(hashingTF.numFeatures, [10000, 50000])\n",
    "             .addGrid(lr.regParam, [0.1, 0.01])\n",
    "             .build())\n",
    "\n",
    "\n",
    "# 5. Set up the Cross-Validator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "\n",
    "# 6. Split data and run the Cross-Validation to find the best model\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"\\nüß† Starting cross-validation... (This will take a long time)\")\n",
    "cvModel = crossval.fit(training_data)\n",
    "print(\"‚úÖ Cross-validation complete. Best model found.\")\n",
    "\n",
    "\n",
    "# 7. Use the BEST model found by the CrossValidator for evaluation\n",
    "print(\"\\nüìä Evaluating the BEST model on the test set...\")\n",
    "bestModel = cvModel.bestModel\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# --- General Evaluation ---\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nüèÖ ROC AUC of Best Model: {roc_auc:.4f}\")\n",
    "\n",
    "# --- Detailed Per-Class Evaluation ---\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Metrics for Negative Reviews (Class 0)\n",
    "precision_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "recall_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "f1_0 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\", multi_evaluator.metricLabel: 0.0})\n",
    "\n",
    "# Metrics for Positive Reviews (Class 1)\n",
    "precision_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 1.0})\n",
    "recall_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 1.0})\n",
    "f1_1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\", multi_evaluator.metricLabel: 1.0})\n",
    "\n",
    "print(\"\\n--- Metrics for Negative Reviews (Class 0) ---\")\n",
    "print(f\"üéØ Precision: {precision_0:.4f}\")\n",
    "print(f\"üîÅ Recall:    {recall_0:.4f}\")\n",
    "print(f\"üéØ F1 Score:  {f1_0:.4f}\")\n",
    "\n",
    "print(\"\\n--- Metrics for Positive Reviews (Class 1) ---\")\n",
    "print(f\"üéØ Precision: {precision_1:.4f}\")\n",
    "print(f\"üîÅ Recall:    {recall_1:.4f}\")\n",
    "print(f\"üéØ F1 Score:  {f1_1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\\nüìã Confusion Matrix\")\n",
    "print(\"Rows: True Label, Columns: Predicted Label\")\n",
    "predictions.groupBy('label').pivot('prediction', [0.0, 1.0]).count().na.fill(0).show()\n",
    "\n",
    "\n",
    "# 8. Save the best model\n",
    "print(\"\\nüíæ Saving the best model to GCS...\")\n",
    "model_path = \"gs://steam-reviews-bucket-0/models/spark_lr_model_tuned\"\n",
    "bestModel.write().overwrite().save(model_path)\n",
    "print(f\"‚úÖ Best model successfully saved to: {model_path}\")\n",
    "\n",
    "\n",
    "# 9. Stop the SparkSession\n",
    "print(\"\\nüõë Stopping the Spark session.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae3d26-8cec-4a49-b0a7-a04e10bdda06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-runtime-0000c9133de0",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "spark kernel on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000c9133de0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
